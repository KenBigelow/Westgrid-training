{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll look at reading in raw data files and working with them. We will read in a sample text file and do a quick frequency analysis on it. Once we have that we'll try to build an application to generate an something that \"looks like\" English. The main points here are\n",
    "\n",
    "  * Learning to work with external data sources\n",
    "  * Learning how to quickly explore your data\n",
    "  * Cleaning your data\n",
    "  * Performing an analysis\n",
    "  * Making models and predictions\n",
    "  \n",
    "Along the way we'll take a basic look at some concepts from information theory such as probablilty and information entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we start with some boilerplate. \n",
    "# Some combination of these imports will appear in almost all \n",
    "# of your notebooks or python code\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Text\n",
    "\n",
    "I've stolen most of this analysis shamelessly from a course by Benjamin Schumacher [The Science of Information: From Language to Black Holes](https://thegreatcourses.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ReturnOfSherlockHolmes.txt', 'r') as f:\n",
    "    rawTxt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look to make sure we have the right thing in sourceTxt. The sourceTxt[:100] syntax should give us the first 100 characters of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rawTxt[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that pretty good, but let's throw away the header. We can do this by using Python's string methods to search for the String `I.--The Adventure of the Empty House` and trimming there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookTxt = rawTxt[rawTxt.find('I.--The Adventure of the Empty House.'):]\n",
    "print(bookTxt[:256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good. Now we want to find the occurences of each token that appears in the text. We could write some loops to do this, but with Python someone has usually already thought of this for you. The collections module, deals with collections of things and it has a Counter object which will slurp up our `bookTxt` and count each item. s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(bookTxt)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good, but there here are a few things we want to fix. We only want to deal with the letter frequency so we want 'x' and 'X' to be considered as the same item. Python strings have a `lower` method which will translate everything to lowercase. Next, there are some special characters to deal with. We can replace newlines with a space without changing meaning, but for the other characters (e.g. `&, %, ;`)we will just remove them as well to keep things simple. In summary we want to retain `[a-z ]`. Again hitting tab completion on the string methods turns up a `translate` method which allows us to make replacements, we can use this to replace each of the special characters with the special `None` object. The next expression is pretty complicated, but just start on the inside and work your way out to see what it does:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = [' ', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n",
    "          'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "          'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "          'y', 'z']\n",
    "\n",
    "#bookTxt = bookTxt.replace('\\n', ' ').lower().translate(\n",
    "#    {ord(i): None for i in c.keys() if i not in 'abcdefghijklmnopqrstuvwxyz ,.'}\n",
    "#)\n",
    "bookTxt = bookTxt.replace('\\n', ' ').lower().translate(\n",
    "    {ord(i): None for i in c.keys() if i not in letters}\n",
    ")\n",
    "\n",
    "bookTxt[:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Counter(bookTxt)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the data we want to work with. Let's look at a barchart to show the relative frequencies visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(*zip(*s.most_common()), width=.75, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corrsponds with our intuition. The space ' '  is the most common character, then we have 'e', 't', 'a' ...\n",
    "\n",
    "We will make a pandas DataFrame with this information as the first column. DataFrames are very flexible and intuitive objects you'll find in most programming languages when dealing with data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF = pd.DataFrame.from_dict(s, \n",
    "                                  orient='index', \n",
    "                                  columns=['count']\n",
    "                                 ).sort_values(by='count',ascending=False)\n",
    "holmesDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column with the frequency of each letter as a probability, sum up all of the values in the count column, then divide that column by the total to estimate the letter probability. This isn't actually adding new information to the dataFrame (the information content is identical to the existing 'count' column, but it is convenient so let's ignore that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF['rel_freq'] = holmesDF['count'] / holmesDF['count'].sum()\n",
    "holmesDF.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is _almost_ the same distribution when doing the same thing for English in general, but 's' and 'h' are transposed (maybe all those references to 'SHerlock'. The rel_freq values should sum to 1 (within rounding) and we can treat them as a probability distribution for generating \"English\" text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF['rel_freq'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "\n",
    "### $0^{th}$ order model - Uniform probabilities\n",
    "\n",
    "We'll dig into the numpy module to do the selection, specifically the `numpy.random` submodule. One trick I recommend here is to set a `random_seed`. This will make your results reproducible. Most of the time you are dealing with 'pseudo' random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(645)\n",
    "uniTxt = np.random.choice(holmesDF.index, size=10240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join(uniTxt[:2048]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't great, if you try to read through it you'll see that we have lots of relatively uncommon letters `z`, `j` etc. There are also lots of funny combinations of letters. Let's confirm this by creating the same histogram as before for this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Counter(uniTxt)\n",
    "plt.bar(*zip(*s.most_common()), width=.75, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $1^{\\textrm{st}}$ order model\n",
    "We'll we can't complain, this is exactly what we asked for. Let's try the same thing but using the frequencies we calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frqTxt = np.random.choice(holmesDF.index, size=10240, p=holmesDF['rel_freq'])\n",
    "print(''.join(frqTxt[:2048]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already that is starting to look better. If you really squint you might be able to convince yourself that this is some lost dialect of finnish or something. Let's double check the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Counter(frqTxt)\n",
    "plt.bar(*zip(*s.most_common()), width=.75, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we could look at probabilities for pairs or triples of letters and select groups based on those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise\n",
    "\n",
    "Consider some binary system, for example a fire alarm. The fire alarm is an information source with two states. The fire alarm is either on or it is off, telling us there is or isn't a fire. For the vast majority of the time the alarm is (hopefully!) off. Intuitively, the `on` state seems to tell us more than the off state, but we want to be able to quantify that. We can do that with a quantity called surprise which we take to be the log of the reciprocal of the probability. That's quite a mouthful so let's think about what we would want from a measure of \"surprise\" then explore this specific implementation.\n",
    "\n",
    "If something has a very small probability (say p=0.001, then the reciprocal of that is very large (1/p = 1000), so as probabilities get smaller, this quantity gets larger - something we want from a measure of surprise; improbable things are surprising! If something has p=1, then it is certain to happen, you shouldn't be surprised at all, happily $\\log_2(1) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linspace(1e-6, 1, 100, endpoint=False)\n",
    "plt.plot(p, np.log2(1/p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fire example, say that the probability of a fire is $p=0.001$, so the probability of no fire is $q= 1-p =0.999$, then these have surprise\n",
    "$$\n",
    "  S_p = \\log_2(1/0.001) \\approx 9.9658\\\\\n",
    "  S_q = \\log_2(1/0.999) \\approx 0.0014\n",
    "$$\n",
    "(units are bits)\n",
    "\n",
    "### [Hartley-Shannon Entropy](https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem)\n",
    "\n",
    "We can now calculate the surprise for letters in our sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF['surprise'] = np.log2(1/ holmesDF.rel_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $2^{\\textrm{nd}}$ order model\n",
    "\n",
    "Letters aren't independnent in english. A q is far more likely to be followed by a `u` than by any other letters, and there are lots of combinations which we would struggle to pronounce e.g. `hpkrl`. We need to consider the correlations between these letters. Each letter is going to be followed by something (until we get to the end of the text), so if take the letter a, the the sum of probabilitie over all letters should be 1, i.e. $\\sum_{x\\in{ab...z }}  p_A(x) = 1$. For each letter in our source text, we want to count all of the times the next letter is `a`, all of the times it is `b` etc. There are lots of ways to do this, but for me, I'm most comfortable in numpy so I'll turn the book text into an array of letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookLetters = np.array(list(bookTxt))\n",
    "bookLetters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells us whether each element is 'a' or not as a boolean\n",
    "bookLetters == 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nonzero` method translates the `True` values into index positions, so we have the indexes for each occurence of the letter `a` in the text. Adding 1 to these values gives us all of the letters which follow the `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextLetter = bookLetters[np.nonzero(bookLetters == 'a')[0] + 1]\n",
    "nextLetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can count up the occurences for each of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(nextLetter, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing `a` to `q` above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the holmesDF dataframe, I would like to be able to access all of these values so we will add columns for every key in the index, we will 29 new columns with these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in letters:\n",
    "    nextLetter = bookLetters[np.nonzero(bookLetters == character)[0] + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh... I forgot that the last letter of the text isn't followed by anything! We can use numpy indexing to avoid the last character of bookLetters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlCounts = {}\n",
    "for character in letters:\n",
    "    nextLetter = bookLetters[np.nonzero(bookLetters[:-1] == character)[0] + 1]\n",
    "    char, count = np.unique(nextLetter, return_counts=True)\n",
    "    nlCounts[character] = dict(zip(char,count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlCounts['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for firstLetter in letters:\n",
    "    for secondLetter in letters:\n",
    "        if secondLetter in nlCounts[firstLetter]:\n",
    "            holmesDF.loc[firstLetter, secondLetter] = nlCounts[firstLetter][secondLetter]\n",
    "        else:\n",
    "            holmesDF.loc[firstLetter, secondLetter] = 0.\n",
    "\n",
    "holmesDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can translate these into probabilities if we normalize by the letter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holmesDF.loc[:, letters].div(holmesDF['count'], axis='index')\n",
    "holmesDF.loc[:, letters] = holmesDF.loc[:, letters].div(holmesDF['count'], axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of the letter pair probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF.loc[:, letters].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hDF = holmesDF.loc[:, letters].sort_index()\n",
    "plt.pcolor(hDF)\n",
    "plt.yticks(np.arange(0.5, len(hDF.index), 1), hDF.index)\n",
    "plt.xticks(np.arange(0.5, len(hDF.columns), 1), hDF.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a little bit hard to interperate, but on the y-axis we have the first letter, scanning across each row the colours depict the probability of finding the letter in that column next. The bright yellow dot is corresponds to the `q`, `u` association. There are 413 occurrences of `q` in our text and every time, it is followed by a `u`, so the yellow corresponds to a probability of 1. We want our second order model to use these probabilities, so if a q is selected our model should _always_ spit out a `u` next.\n",
    "\n",
    "Our model should work like this: For the first letter make a random choice from with probability `rel_freq`. Say we selected the letter 't'. Next we look at the row with index `t` in `holmesDF` we select the next letter according to the letter probabilities in that row. We keep iterating this second step to generate our sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstLetter = np.random.choice(holmesDF.index, size=1, p=holmesDF['rel_freq'])\n",
    "firstLetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have selected the letter `r`, here are the probabilities for selecting any other char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF.loc['r', letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holmesDF.loc['r', letters].plot(kind='bar', width=.75, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the character `e` is most likely to be selected next, followed by the space character, then `o` etc. Let's iterate this process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be handy to have each of our probability arrays in memory so let's build them from our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letterP = {}\n",
    "for letter in letters:\n",
    "    lSeries = holmesDF.loc[letter, letters]\n",
    "    letterP[letter] = (lSeries / lSeries.sum()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondOut = [firstLetter[0]]\n",
    "\n",
    "for i in range(512):\n",
    "    secondOut.append(\n",
    "        np.random.choice(\n",
    "            list(letterP[secondOut[-1]].keys()), \n",
    "            size=1, \n",
    "            p=list(letterP[secondOut[-1]].values())\n",
    "        )[0])\n",
    "    \n",
    "print(''.join(secondOut))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bile`, `rat`, `wean`, `wave`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
