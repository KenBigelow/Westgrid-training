{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "\n",
    "Pandas (Python Data Analysis Library) is a swiss-army knife module that you'll find at the top of a huge proportion of notebooks. It's popular enough to deserve its own import idiom..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a quick idea of the scope of pandas take a look at the autocomplete for pd.<TAB>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental objects in pandas are the `Series` and the `DataFrame`. Together they encapsulate how you will normally ingest, clean, manipulate and even visualize external data sources.  Most of this module will use methods on these objects so we'll take a quick tour of the concepts they implement, and the idea of an Index.\n",
    "\n",
    "## Series\n",
    "\n",
    "The pandas `Series` object is basically a one dimensional indexed array. Schamatically, they look like\n",
    "\n",
    "| Index | Value |\n",
    "|-------|-------|\n",
    "|   0   |  0.12 |\n",
    "|   1   |  0.24 |\n",
    "|   2   |  0.36 |\n",
    "|   3   |  0.48 |\n",
    "\n",
    "\n",
    "Where there are two columns: an index and a value. Most of the time the index values are distinct (not a firm requirement though!) but they don't have to be integers. Any hashable type will do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series({'one' : 1.0, 'two': 2.0, 'three': 3.0})\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particularly common option is to use a timestamp as the index (don't worry about the syntax here, we'll come back to timeseries later)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti = pd.date_range('2020-06-03', periods=3, freq='H')\n",
    "pd.Series(['first', 'second', 'third'], index=dti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to notice is that the values all have the same type (`dtype: float64` in this case). `pandas` can make `Series` of almost any type as long as all the rows share the same type, but it will try to pick the most efficient implementation (`numpy` `ndarrays` for numeric types). It can fall back to a generic `object` type if all else fails but it really pays (in speed) to keep an eye on the `dtype` and make it a `numpy` type if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.Series(['one', 2, 'three'])\n",
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making `Series`\n",
    "\n",
    "You'll probably find that you are making `DataFrames` more often than `Series`, but lots of operations in `panadas` return Series or want them as an argument so it's good to know how to build them.\n",
    "\n",
    "Like `numpy` `ndarray`s a sequence will work, it'll get the default ascending integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pd.Series([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = pd.Series(range(5))\n",
    "si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__repr__` includes the values we set along with the type of object we have stored (the values). As I mentioned, this is one of the attributes of a Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this looks a lot like a numpy array (or even just a list), but we can switch the indexing to suit our needs, by explicitly passing the `index=` arguement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = pd.Series([1., 2., 3., 4., 5.], index=['one', 'two', 'three', 'four', 'five'])\n",
    "sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm['three']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the series is starting to look more like a dictionary, in fact, that's a pretty good way to construct series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = pd.Series({'one': 1, 'three': 3, 'two': 2, 'four': 4, 'five': 5})\n",
    "sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look closely though, a Series has a few tricks that a dictionary doesn't..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn['three':'four']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. label based indexes are _inclusive_ of the `stop` value. This is different from most other indexes you'll see in python, but this can cause a little confusion. Basically it boils down to the idea that there isn't always a natural \"next\" object in a hash (notice that the rows aren't sorted beyond what we specified manually in the index).\n",
    "\n",
    "If you use numbers to do the slice, you'll get the familiar python behaviour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series have a `keys()` method, but it returns an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element by element statements evaluate to Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn > 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if you remember the material on numpy fancy indexing, this can be very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn[sn > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some extr indexing methods available to you `.loc`, `.iloc`, `.ix`. Have a look at the documentation for these, they might seem redundant, but actually they're useful in some contexts where ordinary indexing will bite you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = pd.Series(data=['apple', 'banana', 'orange', 'pineapple'], index=[3, 2, 1, 7])\n",
    "\n",
    "sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want the value in the 4th row..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, `3` was interperted as a label. There's an ambiguity because of the type of the index. Fortunately `pandas` gives us a pair of functions to avoid that problem\n",
    "\n",
    "  * `.iloc[]`: Purely integer-location based indexing.\n",
    "  * `loc[]` : Purely label based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `.iloc` will also work with slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the opposite situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.loc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iloc` does the opposite and tells pandas you want to use the implicit style python notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.iloc[[2, 1, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ix` some combination of these things, but I've never been able to commit it to memory. Fortunately it is being deprecated in favour of `iloc` and `loc` so you don't need to worry ðŸ˜€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrames`\n",
    "\n",
    "Most of the time you will be using `DataFrames` rather than `Series`, but at a first pass it is OK to think of `DataFrames` as a bunch of `Series` stuck together with a common index. \n",
    "\n",
    "\n",
    "| Index | Value1 | Value2 | \n",
    "|-------|--------|--------|\n",
    "|   0   |  0.12  |   'a'  |\n",
    "|   1   |  0.24  |   'b'  |\n",
    "|   2   |  0.36  |   'c'  |\n",
    "|   3   |  0.48  |   'd'  |\n",
    "\n",
    "The rules discussed above for the index stay the same but notice that we can now have different types in the various columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame({'floats': sm, 'ints': sn})\n",
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Attributes\n",
    "\n",
    "We've already seen some of the attributes of the DataFrame (column etc.) but there are quite a few available, take a look at `dtypes`, `ndim`, `shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "When indexing a dataframe, the default is to give you the column (you can also use the syntax `d1.ints`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['ints']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are looking for the row, then try `.loc` with the row index value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.loc['one']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with series, you can give a list instead, but remember to count the parentheses..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.loc[['one','two']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wherever possible, `pandas` (like `numpy`) will try to return a view on the same data rather than a copy, but because the indexing possibilities in `pandas` are much greater the specific rules are more subtle and we won't get into them now, suffice to say, if you need to break the link between two dataframes you can use the `.copy()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1a = d1\n",
    "d1a is d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1b = d1.copy()\n",
    "d1b is d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing DataFrames\n",
    "\n",
    "`DataFrames` are mutable; we can change the values in rows and columns and we can add/remove columns in place. `pandas` will usually try to do this in place, but some modifications (e.g. changing column dtype) require implicit data copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.iloc[1] = (3.0, 3)\n",
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['ratio'] = d1['ints'] / (2 * d1['floats'])\n",
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes not everything will fit in a single `DataFrame`, we'll see how to concatenate and otherwise join `DataFrames` later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Methods\n",
    "\n",
    "There are *lots* of methods for operating on DataFrames, have a look at the tab completion and explore the documentation for them. In particular, take a look at the help for `describe`, `head` and `tail`. These are great for orienting yourself with a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't have time to dive into all of the methods but we'll sample a few, then you should explore the documentation for others. Some of my favourites are\n",
    "\n",
    "* `min`, `max`\n",
    "* `mean`, `mode`, `median`\n",
    "* `max`, `min`, `argmin`, `argmax`, `idxmax`, `idxmin`\n",
    "* `any`, `all`\n",
    "* `astype`\n",
    "* `dropna`\n",
    "* `shift`\n",
    "* `sort_index`, `sort_values`\n",
    "\n",
    "Generally these will return another `DataFrame` with the results you are looking for, but you can also pass the `inplace=True` keyword argument which will modify the `DataFrame` in place and save some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d1 > 3).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about that 1, `d1 > 3` returns a dataframe of booleans, any checks if there are any true values in each of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['floats'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.sort_values('floats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dropna` comes in VERY handy in combinations with other functions. For example, if your dataframe has a `NaN` in one row of a column, calling `.mean()` on that column won't work, but if you know that value can be safely ignored, you can do `.dropna().mean()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with External Data\n",
    "\n",
    "Data comes in many, many forms from simple csv/json files, real-time APIs, structured binary files and many others. Try `pd.read_<TAB>` to see some of the `pandas` igestion options. `read_csv` is the main workhorse for data sets which will fit on a single machines. It is way more flexible than it's name suggests (S3 buckets, https, compressed files, ...) and many of the arguments to `read_csv` will have equivalents for the other functions, so we'll take a closer look at.\n",
    "    \n",
    "We need a CSV to work with. The city of Vancouver has an [open data catalog](https://vancouver.ca/your-government/open-data-catalogue.aspx), which has CSV for some of the datasets. There's a dataset which lists all of the community gardens and food trees maintained by the city. A copy of it is available in this directory called `CommunityGardensAndFoodTrees.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF = pd.read_csv(\n",
    "    \"CommunityGardensAndFoodTrees.csv\",\n",
    "    encoding='latin1',\n",
    "    delimiter=';'\n",
    ")\n",
    "gardenDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 172 rows, with 19 columns, here are the fist few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can tell things like the gardens were created (`YEAR_CREATED`), and where the are (`LATITUDE`, `LONGITUDE`), and who's responsible for them (`STEWARD_OR_MANAGING_ORGANIZATION`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to clean the data. This is a hugely important step and will generally eat a lot of your time, but it is worth doing right. Having mistakes in your data undermine everything you are trying to do.\n",
    "\n",
    "First let's look at the index, the default is to index by integer, but we could have picked any column instead. It looks like the first column is unique (`MAPID`) and so let's use that (chosing the index right can make your life much easier when adding data or combining multiple DataFrames). The `inplace=True` argument means modify the existing dataframe rather than returning a modified copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF.set_index('MAPID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful trick when cleaning data is to look at the unique values in a column. You'll often catch coding mistakes or values being used as placeholders this way, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['YEAR_CREATED'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pre-2010`, `pre-1970` and `pre 2000` are kind of usless (and inconsistent!) so let's toss them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for badLabel in ['Pre-2010', 'pre-1970', 'pre 2000']:\n",
    "    gardenDF = gardenDF[gardenDF['YEAR_CREATED'] != badLabel]\n",
    "\n",
    "gardenDF['YEAR_CREATED'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still a `nan`, and the years are strings (numbers would be better, or even dates). Pandas is pretty smart about dealing with missing data, but that isn't enough there are methods like `dropna()` which will tell pandas to remove them from results or you can remove the problem entries with fancy indexing. For exampe, if we try to convert the `YEAR_CREATED` column to an integer blindly, it will barf on the `nan` (`NaN` is defined for floats but not ints, blame the IEEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['YEAR_CREATED'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can use the `.notnull` method to figure out where those rows are. This will give us a boolean array which we can use for Fancy Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['YEAR_CREATED'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF = gardenDF[gardenDF['YEAR_CREATED'].notnull()]\n",
    "gardenDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could make `YEAR_CREATED` an integer (or date, see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['YEAR_CREATED'] = gardenDF['YEAR_CREATED'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['YEAR_CREATED'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column dtype *is* int64, but `describe()` wants floats to work with so it gets converted.\n",
    "\n",
    "Doing these steps every time for each column can be a lot of work and code, fortunately `read_csv` (and the other ingestion methods) can do most of the work while we're reading in the data. \n",
    "\n",
    "  * **delimiter=**: Sometimes a csv is a tsv, tabs are evil\n",
    "  * **names=**: Pass a list of names to use for the columns\n",
    "  * **usecols=**: Only slurp up a subset of columns\n",
    "  * **skiprows=**: Ignore a number of rows at the top of the file\n",
    "  * **na_values=**: Flag values which the CSV author used to indicate missing data, e.g. -1\n",
    "  * **encoding=**: ...\n",
    "  * **converters=**: Do some transformation on the columns before importing them\n",
    "  * **parse_dates**=: Turn strings into dates\n",
    " \n",
    "Some of these are obvious (e.g. pass in a list of names) but if you look at the documentation they are *really* flexible. Many will accept combinations of columns, functions, dictionaries of functions, etc. For dates you might have years in one column, months in another and days in a third. `parse_date` can combine these into a single `DateTime` object. Other times you might want to transform a column (e.g. convert to int or subtract off a constant), `converters` lets you specify functions to be run on columns during the ingestion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "geom = gardenDF.loc['FA008']['Geom']\n",
    "json.loads(geom)['coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def geom2lat(geom):\n",
    "    if geom:\n",
    "        return json.loads(geom)['coordinates'][0]\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "\n",
    "gardenDF = pd.read_csv(\n",
    "    \"CommunityGardensAndFoodTrees.csv\",\n",
    "    usecols = [\n",
    "        'MAPID',\n",
    "        'YEAR_CREATED',\n",
    "        'NAME',\n",
    "        'STEWARD_OR_MANAGING_ORGANIZATION',\n",
    "        'STREET_NUMBER',\n",
    "        'STREET_NAME',\n",
    "        'Geom',\n",
    "        'Geom'\n",
    "    ],\n",
    "    delimiter=';',\n",
    "    encoding='latin1',\n",
    "    na_values={\n",
    "        'YEAR_CREATED': ['Pre-2010', 'pre-1970', 'pre 2000', 'nan']\n",
    "    },\n",
    "    index_col='MAPID',\n",
    "    parse_dates=['YEAR_CREATED'],\n",
    "    converters={\n",
    "        'Geom': geom2lat,\n",
    "    }\n",
    ")\n",
    "gardenDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same thing but with JSON/API\n",
    "\n",
    "[opendata vancouver](https://opendata.vancouver.ca/api/v2/console) publishes the same datasets as an API. The requests module lets you talk to that API and gives you results in JSON to play with. `pandas` can then consume that JSON, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_url = 'https://opendata.vancouver.ca/api/records/1.0/search/'\n",
    "headers = {\n",
    " 'Content-Type': 'application/json; charset=utf-8'\n",
    "}\n",
    "\n",
    "\n",
    "params = {\n",
    "    'dataset' : 'community-gardens-and-food-trees',\n",
    "    'q'       : '',\n",
    "    'rows'    : 10,\n",
    "    'facets'  : ['year_created', 'juristiction', 'geo_local_area']\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.get(base_url, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()['records'][0]['fields']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can do something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(r.json()['records'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives us the first 10 rows of our dataframe. Depending on the details of the API (`next_url`, rate limits etc.) the next step would be to make an iterable list of these results and the concatenate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeries\n",
    "\n",
    "We've already talked about time and date handling a bit, but I use this a lot, so we can talk about it more! Pandas was created to handle Financial data and do financial modeling. This lineage has given pandas really excellent time handling. The main objects are\n",
    "\n",
    "  * **Time Stamps**: Specific points in time usually recorded to the second or nanosecond\n",
    "  * **Time Intervals/Time Deltas**: These types lets you do arithmetic on time objects\n",
    "\n",
    "We need some dates to play with. There's a convenience function called `to_datetime` which can convert many \"human readable\" dates to a pd.Timestamp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "moonwalk = pd.to_datetime('July 20, 1969, 20:17 UTC')\n",
    "moonwalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamps have attributes which let you extract days, year, etc. Normally these will be reported as numbers, but the strftime method supports the usual format specifiers (The correspond with the libc specifiers, here's a reference http://strftime.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The moon walk took place on a {moonwalk.strftime('%A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(datetime.utcnow(), utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another sample dataset, it contains three colums\n",
    "\n",
    "  * year\n",
    "  * month\n",
    "  * passengers\n",
    "  \n",
    "We can combine the year and month to create a date, then we can use the result as the index for a single column dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/flights.csv',\n",
    "    parse_dates=[['year','month']],\n",
    "    index_col='year_month'\n",
    ")\n",
    "flightsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can index based on date. Let's look at the number of flights in the 1951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF['1951':'1952'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamp objects can also deal with arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.index[-1] - flightsDF.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2019-06-24 09:00', periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extremely useful feature with time series is the ability to resample existing time series. For example, we could resample the flight data into year long bins and look at how the mean passenger count increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.resample('Y').mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping, Joining, Concatenating\n",
    "\n",
    "You can get pretty far by jamming everything into a single dataframe, but sometimes you might want to do aggregate operations within a dataframe (e.g. group together all of the rows by year and show the mean value of some other column). Alternatively you might want to add new rows to or columns to an existing DataFrame or join dataframes based on shared key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv'\n",
    ")\n",
    "carsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "Let's group things by number_of_cylinders and see how that affects mpg..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF['mpg'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling groupby on it's own will give you a `DataFrameGroupBy` object, you have to tell it what you want to do with the groups to actually see some results, this can be convenient if you want to look at different aggregate functions on the same groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders = carsDF.groupby('cylinders')\n",
    "carsDFbyCylinders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical aggregate operations are things like\n",
    "\n",
    "  * mean()\n",
    "  * sum()\n",
    "  * median()\n",
    "  * min()/max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the carsDF by model_year and look at the median mpg (don't include the other columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF.groupby('model_year')['mpg'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can actually do much more with groupby, you can iterate over the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vroom, group in carsDFbyCylinders:\n",
    "    print(f\"There are {group.shape[0]} cars with {vroom} cylinders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are there possibly 4 cars with 3 cylinders?!\n",
    "\n",
    "We can apply multiple operations at the same time. The `.aggregate()` method can take a list of the operations you want to perform (e.g. [\"max\", \"min\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders['mpg'].aggregate([\"min\",\"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter based on group, this is a silly example, but group the cars by cylinder, then show me all of the groups with a mean mpg > 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders.filter(lambda x: x['mpg'].mean()>15).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform method lets you perform a group operation then use the results to update the rows. For example, we could calculate mean values for our groups, then look at how individual cars perform relative to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders.transform(lambda x: x - x.mean()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an apply method which is even more general and will let you apply an arbitrary function to the group results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate & Join\n",
    "\n",
    "There are a handful of functions which handle concatenation. The main workhorse is `pd.concat`, but there are some convenience functions which will let you avoid passing lots of arguments to concat. You can combine either `Series` and `DataFrames` but we'll jump straight to `DataFrames`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.DataFrame(\n",
    "    {\n",
    "        'upper': ['A', 'B', 'C'], \n",
    "        'lower': ['a', 'b', 'c']\n",
    "    }, \n",
    "    columns=['upper', 'lower'], \n",
    "    index=[1,2,3]\n",
    ")\n",
    "\n",
    "s2 = pd.DataFrame(\n",
    "    {\n",
    "        'upper': ['D', 'E', 'F'],\n",
    "        'lower': ['d', 'e', 'f']\n",
    "    }, \n",
    "    columns=['upper', 'lower'],\n",
    "    index=[4,5,6]\n",
    ")\n",
    "\n",
    "pd.concat([s1,s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we enclosed the thigs we want to join as some sort of iterable (a `list` here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to add columns rather than rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.DataFrame({'upper': ['A', 'B', 'C'], 'lower': ['a', 'b', 'c']}, columns=['upper', 'lower'], index=[1,2,3])\n",
    "t2 = pd.DataFrame({'greek': ['Î±', 'Î²', 'Î³']}, index=[1,2,3])\n",
    "\n",
    "pd.concat([t1, t2], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.concat` will accept duplicate indices, but normally that indicates a problem with the data normalization. `concat` has a `verify_index` argument which can check for these problems and you can specify what you want to do with duplicates manually.\n",
    "\n",
    "concat will often result in `NaN`s because some columns might not exist in both/all frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = pd.DataFrame({'fruit': ['apple', 'orange'], 'veg': ['brocolli', 'carrot'], 'tree': ['cedar', 'alder']})\n",
    "d5 = pd.DataFrame({'veg': ['onion', 'potato'], 'fruit': ['banana','grape']})\n",
    "pd.concat([d4,d5], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the index wasn't important here, I threw it away and just accepted a new one.\n",
    "\n",
    "In the general case, joining DataFrames can get complex. The concat method can take a `join` keyword to specify a database like join stragegy (inner or outer), but `pd.merge` is a bit more flexible. It implements the usual relations\n",
    "\n",
    "  * one-to-one (similar to a concat)\n",
    "  * many-to-one\n",
    "  * many-to-many\n",
    "  \n",
    "For the many-to-one case here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf1=pd.DataFrame({\n",
    "    'class': ['insect', 'spider'], \n",
    "    'legs': [6, 8]}\n",
    ")\n",
    "\n",
    "adf2=pd.DataFrame({\n",
    "    'name': ['molly', 'anna', 'stephen', 'mica'], \n",
    "    'class': ['insect','insect','spider','insect']}\n",
    ")                     \n",
    "\n",
    "print(adf1); print(adf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(adf1, adf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _many-to-one_ is many different rows in adf2 being mapped to a single row in adf1 (insects). `pd.merge` also accepts a selection of keyword arguments so you can manually specify which columns to join, patch up name differences etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
